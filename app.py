from flask import Flask, request, jsonify
import torch
from transformers import BertTokenizerFast, BertForTokenClassification
import numpy as np
import pickle


app = Flask(__name__)


# Check for devices
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Server is running on {device}')

# Load the model from path (folder)
model_name = 'model-v5-test'
model_path = f'{model_name}'
tokenizer = BertTokenizerFast.from_pretrained(model_path)
model = BertForTokenClassification.from_pretrained(model_path)
model.to(device)
#print(model)

# Load ids and labels --> improvement: define them
with open(f'{model_path}/labels_to_ids_{model_name}.pkl', 'rb') as f:
    labels_to_ids = pickle.load(f)
with open(f'{model_path}/ids_to_labels_{model_name}.pkl', 'rb') as f:
    ids_to_labels = pickle.load(f)

# Testing route
@app.route('/', methods=['POST'])
def parse_job_title():
    # GET DATA
    data = request.get_json()  # get Data from the post request
    if not data:
        return jsonify({'error': 'No data provided'}), 400
    job_titles = data.get('jobTitles')      # RESPONSE key 0: original job titles
    ids = data.get('ids')

    # PRE-PROCESS DATA
    job_titles_splitted = []                # RESPONSE key 1: job titles splitted as they enter to the model
    for sentence in job_titles[:]:
        if not isinstance(sentence, str):
            sentence = str(sentence)
        delimiters = ['-', '/', '_', ',', '.']
        for delimiter in delimiters:
            sentence = sentence.replace(delimiter, f' {delimiter} ') # Separate special characters to facilitate labelling
        job_titles_splitted.append(sentence.split()) # append split words
    
    # CLASSIFY PROCESSED DATA (iterator) --> Improvement: build a DataLoader
    job_titles_tokenized = []               # RESPONSE key 2: job titles tokenized (turned into word pieces)
    labels_for_tokens = []                  # RESPONSE key 3: labels for tokens (word pieces)
    labels_for_words = []                   # RESPONSE key 4: labels for words

    for job_title_splitted in job_titles_splitted:

        # Turn words into word pieces (tokens), encode and move to device
        inputs = tokenizer(job_title_splitted,
                       is_split_into_words=True,
                       return_offsets_mapping=True,
                       padding='max_length',
                       truncation=True,
                       max_length=128,
                       return_tensors="pt")
        ids = inputs["input_ids"].to(device) 
        mask = inputs["attention_mask"].to(device)
        
        # Turn the ids back to tokens (includes padding) to keep track in the response
        tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist()) 
        word_pieces = []
        for token, mapping in zip(tokens, inputs["offset_mapping"].squeeze().tolist()):
            if mapping[1] != 0: # excludes padding "[PAD]" and special tokens "[CLS]" & "[SEP]"
                word_pieces.append(token) 
        job_titles_tokenized.append(word_pieces) # append word pieces


        # Predict labels for word pieces and padding
        with torch.no_grad():
            outputs = model(ids, attention_mask=mask)
        logits = outputs.logits
        active_logits = logits.view(-1, model.num_labels) 
        flattened_predictions = torch.argmax(active_logits, axis=1) # get the most confident label
        
        token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()] 
        wp_preds = list(zip(tokens, token_predictions)) # this includes preds for padding

        # Filter to keep labels for word pieces (bert tokens)
        tokens_prediction = []
        for token_pred, mapping in zip(wp_preds, inputs["offset_mapping"].squeeze().tolist()):
            if mapping[1] != 0: # excludes padding "[PAD]" and special tokens "[CLS]" & "[SEP]"
                tokens_prediction.append(token_pred[1])
        labels_for_tokens.append(tokens_prediction)  # append labels for tokens (word pieces)
        

        # Filter to keep labels for words (splitted sentence)
        words_prediction = []
        for token_pred, mapping in zip(wp_preds, inputs["offset_mapping"].squeeze().tolist()):
            if mapping[0] == 0 and mapping[1] != 0: # first condition excludes word pieces generated by BertTokenizer
                words_prediction.append(token_pred[1])
        labels_for_words.append(words_prediction)


    # BUILD RESPONSE OBJECT 
    response_data = {
        'ids': ids,
        'job_titles': job_titles,
        'job_titles_splitted': job_titles_splitted,
        'job_titles_tokenized': job_titles_tokenized,
        'labels_for_tokens': labels_for_tokens,
        'labels_for_words': labels_for_words,
    }
    
    return jsonify(response_data), 200

if __name__ == '__main__':
    app.run(debug=True)